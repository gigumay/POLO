{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46d4c92",
   "metadata": {},
   "source": [
    "### Load dependencies \n",
    "We start by importing all necessary python packages, and the functionalities implemented in the utils folder of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from utils.inference_POLO import *\n",
    "from utils.model_eval import *\n",
    "from utils.processing_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c82e2",
   "metadata": {},
   "source": [
    "### Insert dataset name, output directories, etc. \n",
    "Here we specify the dataset we want to test, and set the some paths and inference parameters. Below you can find short explanations for each parameter. \n",
    "- `imgs_dir`: Path to the directory containing the images you want to run inference on.\n",
    "- `output_dir`: Path to the directory ourtputs can be stored in (must exist, is NOT going to be created).\n",
    "- `mdl_path`: Path to the trained model file (.pt).\n",
    "- `device`: Device on which to load and run the model. E.g., `\"0\"`. Pass `\"cpu\"` if you do not have a GPU. \n",
    "- `patch_dims`: Dictionary specifying the tile dimensions.\n",
    "- `ovrlp`: Amount of overlap between tiles (fraction of the tile size. E.g., if tiles are 640x640 and overlap == 0.2, tiles will overlap by 128 pixels).\n",
    "- `dor_thresh`: DoR threshold to use during inference. Can be different from what was used during training. Will affect performance metrics.\n",
    "- `radii`: Dictionary mapping category ID to radius in pixels. Can also be different from training and will also affect performance.\n",
    "- `img_format`: Format extension of the images inference will be perfromed on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_dir = \"\"\n",
    "output_dir = \"\" \n",
    "mdl_path = \"\"\n",
    "device = \"0\"\n",
    "patch_dims = {\"width\": 640, \"height\": 640}  \n",
    "ovrlp = 0.2  \n",
    "dor_thresh = 0.3  \n",
    "radii = {0: 50, 1: 80}\n",
    "classID2name = {0: \"example1\", 1: \"example2\"}\n",
    "img_format = \"jpg\"\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978d401",
   "metadata": {},
   "source": [
    "### Optional: Annotation file and -format\n",
    "During tiled inference, one has the option to collect evaluation metrics and counting erros for each image. However, this requires creating a .json file containing a dictionary that maps image names (file name of the image without format extension) to a list of all the ground truth labels in that image. Annotations must be dictionaries, containing at least the following two keyowrds: `point`, and `category_id`. The value behind the \"point\" key is expected to be a list storing the x- and y-coordinate of the label, whereas for `category_id` the integer class ID must be provided as value:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"img1\": [\n",
    "                {\"point\": [x1, y1], \"category_id\": 0},\n",
    "                {\"point\": [x2, y2], \"category_id\": 2},\n",
    "                ...\n",
    "            ],\n",
    "    \"img2\": [\n",
    "                {\"point\": [x3, y3], \"category_id\": 1},\n",
    "                {\"point\": [x4, y4], \"category_id\": 1},\n",
    "                ...\n",
    "            ],\n",
    "    ...\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to None or remove from below method call if you don't have an annotation file\n",
    "ann_file = f\"\"\n",
    "ann_format = \"PT_DEFAULT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bbc734",
   "metadata": {},
   "source": [
    "### Run tiled inference \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d32ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_tiled_inference_POLO(model=mdl_path, \n",
    "                         class_ids=list(radii.keys()),\n",
    "                         imgs_dir=imgs_dir, \n",
    "                         img_files_ext=img_format,\n",
    "                         patch_dims=patch_dims, \n",
    "                         patch_overlap=ovrlp, \n",
    "                         output_dir=output_dir,\n",
    "                         dor_thresh=dor_thresh,\n",
    "                         radii=radii,\n",
    "                         ann_file=ann_file,\n",
    "                         ann_format=ann_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757e7cd",
   "metadata": {},
   "source": [
    "### Compute Evaluation metrics\n",
    "If annotation-file and -format were passed to `run_tiled_inference_POLO()` above, this cell can be used to evaluated the inference results. It will generate a number of files:\n",
    "- `count_diffs_img_lvl.xlsx`: Excel sheet containing the difference between predicted and ground truth count for each image.\n",
    "- `counts_gt_pred_*.png`: Plot of predicted vs. forund truth count for class `*`.\n",
    "- `counts_total.json`: Predicted counts summed over all images.\n",
    "- `em.json`: Evaluation metrics.\n",
    "- `errors_img_lvl.json`: Counting metrics.\n",
    "- `F1_curve.png`: F1 score plotted against the confidence threshold.\n",
    "- `P_curve.png`: Precision plotted against the confidence threshold.\n",
    "- `R_curve.png`: Recall plotted against the confidence threshold.\n",
    "\n",
    "Calling `compute_errors_img_lvl()` will compute countign errors per image (MAE, MSE, etc.), whereas `compute_em_img_lvl()` will calculate traditional detection metrics like precision and recall. Note that `compute_errors_img_lvl()` will look for a directory called `image_counts`  (which you will have to create) within the directory you specify under the `imgs_dir` parameter. The `image_counts` directory must contain one .json file for each image in `imgs_dir` named exactly like the image (`img1.jpg` --> `img1.json`). The .json file in turn must contain a dictionary mapping class IDs to counts. For example, if `img1.jpg` contains 5 objects of class 0 and no objects of any other class, the dictionary in `img1.json` should look as follows: `{0: 5}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_errors_img_lvl(gt_counts_dir=f\"{imgs_dir}/image_counts\", pred_counts_dir=f\"{output_dir}/detections\", class_ids=list(classID2name.keys()), \n",
    "                           output_dir=output_dir)\n",
    "compute_em_img_lvl(preds_dir=f\"{output_dir}/detections\", class_id2name=classID2name, task=\"locate\", output_dir=output_dir)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P0_YOLOcate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
